{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Uploading the Dataset from my Drive"
      ],
      "metadata": {
        "id": "xkYQ3SF2879v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! gdown https://drive.google.com/drive/folders/1eJPlwS6bd_3SUlm-rh_t7KglTaR4zrIQ?usp=sharing\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "ZYRjSknp88bH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e23d49d-fb25-4fac-8092-9b4939080c49"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "JMMrSVMU8wXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "sStIQ9yx8whb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the DataSet, it spelt to: training/testing/validation data"
      ],
      "metadata": {
        "id": "c5Jmunk48tUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 150\n",
        "categories = [\"NORMAL\", \"PNEUMONIA\"]\n",
        "\n",
        "def create_data(data_path=\"/content/gdrive/MyDrive/chest_xray/train\"):\n",
        "    Created_data = []\n",
        "    for category in categories:\n",
        "\n",
        "        path = os.path.join(data_path,category)\n",
        "        class_num = categories.index(category) \n",
        "        count = 0\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            count += 1\n",
        "            if class_num == 1 and count == 1352:\n",
        "                break\n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)\n",
        "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE)) \n",
        "                if class_num == 0:\n",
        "                    array = [1, 0]\n",
        "                else:\n",
        "                    array = [0, 1]\n",
        "                Created_data.append([new_array, array])\n",
        "            except Exception as e: \n",
        "                pass\n",
        "\n",
        "    return Created_data\n",
        "\n",
        "print(\"Creating training Data: \")\n",
        "training_data = create_data(\"/content/gdrive/MyDrive/chest_xray/train\")\n",
        "\n",
        "print(\"Creating testing Data: \")\n",
        "test_data = create_data(\"/content/gdrive/MyDrive/chest_xray/test\")\n",
        "\n",
        "print(\"Creating validation Data: \")\n",
        "val_data = create_data(\"/content/gdrive/MyDrive/chest_xray/val\")"
      ],
      "metadata": {
        "id": "pt5RMRsC7Or1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea92187c-9a72-42cc-f292-fc602ec76a61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating training Data: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1352/1352 [00:44<00:00, 30.27it/s]\n",
            " 35%|███▍      | 1351/3876 [01:42<03:11, 13.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating testing Data: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 244/244 [00:07<00:00, 30.99it/s]\n",
            "100%|██████████| 390/390 [00:08<00:00, 43.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating validation Data: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:03<00:00,  2.99it/s]\n",
            "100%|██████████| 9/9 [00:01<00:00,  4.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffling the Data to purpose of reducing variance and making sure that models remain general and overfit less."
      ],
      "metadata": {
        "id": "bCG16u7rYcoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(training_data)\n",
        "random.shuffle(test_data)\n",
        "random.shuffle(val_data)"
      ],
      "metadata": {
        "id": "VCAlpQgf7PBh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spilting data into categories"
      ],
      "metadata": {
        "id": "NUC7AGnkaidx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, x_val = [], [], []\n",
        "y_train, y_test, y_val = [], [], []\n",
        "\n",
        "for features,label in training_data:\n",
        "    x_train.append(features)\n",
        "    y_train.append(label)\n",
        "    \n",
        "for features,label in test_data:\n",
        "    x_test.append(features)\n",
        "    y_test.append(label)\n",
        "\n",
        "for features,label in val_data:\n",
        "    x_val.append(features)\n",
        "    y_val.append(label)"
      ],
      "metadata": {
        "id": "VwUOa0ma7PEe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving data model for using it in another algorithms"
      ],
      "metadata": {
        "id": "_LQsgoMmatBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"/content/saved_data/x_train.npy\", x_train)\n",
        "np.save(\"/content/saved_data/y_train.npy\", y_train)\n",
        "\n",
        "np.save(\"/content/saved_data/x_test.npy\", x_test)\n",
        "np.save(\"/content/saved_data/y_test.npy\", y_test)\n",
        "\n",
        "np.save(\"/content/saved_data/x_val.npy\", x_val)\n",
        "np.save(\"/content/saved_data/y_val.npy\", y_val)"
      ],
      "metadata": {
        "id": "IpS-X4VEatpD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "reshaping the data to maximize the accuracy"
      ],
      "metadata": {
        "id": "lRvs0SjyauI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(x_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.array(x_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "x_val = np.array(x_val).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "x_train = x_train/255.\n",
        "x_test = x_test/255.\n",
        "x_val = x_val/255."
      ],
      "metadata": {
        "id": "qUUc79Ms7PJz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running CNN algorithm and checking model accuracy"
      ],
      "metadata": {
        "id": "ltEdM9GYeJe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.load(\"/content/gdrive/MyDrive/saved_data1/x_train.npy\", allow_pickle=True)\n",
        "x_test = np.load(\"/content/gdrive/MyDrive/saved_data1/x_test.npy\", allow_pickle=True)\n",
        "y_train = np.load(\"/content/gdrive/MyDrive/saved_data1/y_train.npy\", allow_pickle=True)\n",
        "y_test = np.load(\"/content/gdrive/MyDrive/saved_data1/y_test.npy\", allow_pickle=True)\n",
        "y_val = np.load(\"/content/gdrive/MyDrive/saved_data1/y_val.npy\", allow_pickle=True)\n",
        "y_val = np.load(\"/content/gdrive/MyDrive/saved_data1/y_val.npy\", allow_pickle=True)\n",
        "\n",
        "x_train = np.array(x_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.array(x_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "x_val = np.array(x_val).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "x_train = x_train/255.\n",
        "x_test = x_test/255.\n",
        "x_val = x_val/255."
      ],
      "metadata": {
        "id": "l2pBKZfnWUMu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', padding=\"same\", input_shape=x_train.shape[1:]))\n",
        "model.add(Conv2D(16, (3, 3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\", input_shape=x_train.shape[1:]))\n",
        "model.add(Conv2D(32, (3, 3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(96, (3, 3), dilation_rate=(2, 2), activation='relu', padding=\"same\"))\n",
        "model.add(Conv2D(96, (3, 3), padding=\"valid\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), dilation_rate=(2, 2), activation='relu', padding=\"same\"))\n",
        "model.add(Conv2D(128, (3, 3), padding=\"valid\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(2 , activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=8, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "h1DTd6dy7PPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4b62aa-d894-418f-e79c-7a165f8d1148"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "85/85 [==============================] - 184s 2s/step - loss: 0.6874 - accuracy: 0.5453 - val_loss: 1.2094 - val_accuracy: 0.5000\n",
            "Epoch 2/8\n",
            "85/85 [==============================] - 178s 2s/step - loss: 0.4530 - accuracy: 0.8239 - val_loss: 0.3465 - val_accuracy: 0.8125\n",
            "Epoch 3/8\n",
            "85/85 [==============================] - 178s 2s/step - loss: 0.2490 - accuracy: 0.9049 - val_loss: 1.8673 - val_accuracy: 0.5000\n",
            "Epoch 4/8\n",
            "85/85 [==============================] - 178s 2s/step - loss: 0.2046 - accuracy: 0.9268 - val_loss: 0.3358 - val_accuracy: 0.9375\n",
            "Epoch 5/8\n",
            "85/85 [==============================] - 182s 2s/step - loss: 0.1884 - accuracy: 0.9382 - val_loss: 0.8125 - val_accuracy: 0.6875\n",
            "Epoch 6/8\n",
            "85/85 [==============================] - 178s 2s/step - loss: 0.1382 - accuracy: 0.9539 - val_loss: 0.2119 - val_accuracy: 0.8750\n",
            "Epoch 7/8\n",
            "85/85 [==============================] - 180s 2s/step - loss: 0.1030 - accuracy: 0.9636 - val_loss: 0.2072 - val_accuracy: 0.9375\n",
            "Epoch 8/8\n",
            "85/85 [==============================] - 177s 2s/step - loss: 0.0810 - accuracy: 0.9703 - val_loss: 0.0697 - val_accuracy: 0.9375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f87f1e4a310>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}